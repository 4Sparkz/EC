{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Engenharia do Conhecimento 2023/2024\n",
    "\n",
    "## Lab class TP06\n",
    "\n",
    "# The K-nearest Neighbour classifier and preprocessing data\n",
    "\n",
    "*A Machine Learning Tutorial by Andre Falcao (DI/FCUL 2020-2022)*\n",
    "*revised by Catia Pesquita (DI/FCUL 2022-2023) and Sofia Teixeira (DI/FCUL 2022-2023)*\n",
    "### Summary\n",
    "\n",
    "\n",
    "1. K-Nearest Neighbours\n",
    "    1. K nearest nighbours for regression\n",
    "    2. K-Nearent neighbours for classification\n",
    "    4. Improving the model with Kernels\n",
    "    \n",
    "3. Scaling Data\n",
    "\n",
    "2. Handling missing values in data\n",
    "\n",
    "\n",
    "## 1. Introduction to K-Nearest Neighbours (KNN)\n",
    "\n",
    "The idea of K nearest neighbours is very simple. It is based on the concept that similar behaviours should appear on items that are near each other. There are thus two issues to solve in KNN\n",
    "\n",
    "1. How many neighbours?\n",
    "2. What is \"near\"?\n",
    "\n",
    "For the current example we will only use the standard Euclidean distance, although it can be changed at will"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. A KNN classifier from scratch\n",
    "\n",
    "A KNN classifier can be simply defined with a class with 3 methods\n",
    "\n",
    "* A constructor that receives the training data and the number of K neighbours\n",
    "* A decision function that for the data from the set of neighbours is able to make a decision. In this case we are going to check the dominant class\n",
    "* A prediction function that will receive a data set of new data and for each will compute the distance between each new instance and all the others in the training set and will return the results from the decision function\n",
    "\n",
    "We are going to use scipy  function `distance_matrix` that compares one data set `te` as a matrix $N_{tr}.M$ with another data set `tr`, $N_{te}.M$ to produce a new matrix $D=N_{tr}.N_{te}$ with the distances from each element `te` to each element of `tr`. This function is able to use all distances produced by the Minkwosky distance, with the default being the Euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance_matrix\n",
    "\n",
    "class knn_classifier:\n",
    "    def __init__(self, X_train, y_train, K=1):\n",
    "        self.data=X_train\n",
    "        self.labels=y_train\n",
    "        self.K=K\n",
    "\n",
    "    def decide(self, results):\n",
    "        #majority decision\n",
    "        vals, cnts=np.unique(results, return_counts=True)\n",
    "        res=list(zip(cnts, vals))\n",
    "        res=sorted(res, reverse=True)\n",
    "        return res[0][1]\n",
    "\n",
    "    def predict(self, instances):\n",
    "        N,M=instances.shape\n",
    "        dst=distance_matrix(self.data, instances)\n",
    "        preds=np.zeros(N, dtype=int)\n",
    "        for i in range(N):\n",
    "            ords=np.argsort(dst[:,i])\n",
    "            res=self.labels[ords[:self.K]]\n",
    "            preds[i]=self.decide(res)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Run the familiar Iris dataset to identify the best possible number of Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "melhor f1 =  1.0\n",
      "melhor quantidade de vizinhos =  15\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
    "\n",
    "\n",
    "X,y=load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "\n",
    "########### Your Solution #############\n",
    "arr = []\n",
    "preds = []\n",
    "\n",
    "for i in range(1, 50):\n",
    "    k = knn_classifier(X_train,y_train,i)\n",
    "    pre =  k.predict(X_test)\n",
    "    sc = f1_score(y_test, pre, average='weighted')\n",
    "    arr.append(sc)\n",
    "\n",
    "m = max(arr)\n",
    "bestK = arr.index(m)\n",
    "print('melhor f1 = ',m)\n",
    "print('melhor quantidade de vizinhos = ', bestK)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Playing with Scikit learn\n",
    "\n",
    "Scikit learn has two powerful KNN implementations for classification and regression, respectively.\n",
    "\n",
    "These implementations allow not only to select the numberr of Neighbours, but allow for different weighting, based on distance, bith for regression and classification. They also allow for using distance modification functions for more complex kernels.\n",
    "\n",
    "As KNN can be a very CPU intensive algorithm, scikit allows for using specific algorithms for searching for similar items  (ball tree and KDTree) as well as using several threads for parallel searching of neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Distance weighting\n",
    "\n",
    "By default KNN regression or classification consideres each neighbour found as contributing equally to the final result, disregarding any effect of the distance. Yet it may convenient to weight each distance to each of the neighbours found according to the inverse of the distance\n",
    "\n",
    "$w_i= \\dfrac{1}{d_i}$\n",
    "\n",
    "Larger distances will have a smaller impact on the inferred value, then closer instances. \n",
    "\n",
    "To visualise the effect of the weight function in regression we are going to use a very simple data problem with 1 feature. The testing set contains 100 X values ranging from 0.0 to 10.0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle\n",
    "X_train, y_train, X_test, y_test = pickle.load(open(\"data.pickle\", \"rb\"))\n",
    "plt.scatter(X_train[:,0], y_train)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to see that the KNN regressor without any weights works in steps, keeping the same value. As long as the neighbours do not change, the prediction will also not change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsRegressor(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "preds = knn.predict(X_test)\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(X_train,y_train, label=\"Training set\")\n",
    "plt.plot(X_test, preds, color=\"darkorange\", label=\"Predictions (K=3)\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With inverse distance weighting, the impact of each nearby point can be seen a a spike."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsRegressor(n_neighbors=3, weights=\"distance\")\n",
    "knn.fit(X_train, y_train)\n",
    "preds = knn.predict(X_test)\n",
    "plt.scatter(X_train,y_train, label=\"Training set\")\n",
    "plt.plot(X_test, preds, color=\"darkorange\", label=\"Predictions\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Kernel Weighting\n",
    "\n",
    "Inverse distance Weighting can be replaced by a sigmoid function, or any other function, for that matter. Replacing the distance with a sigmoid: \n",
    "\n",
    "$w_i= e^{-\\frac{d_i^2}{kw}}$\n",
    "\n",
    "This function will require one further parameter, `kw`, a kernel weight function that can be fit for whatever purpose. On any case, this softer weighting will create a better interpolation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian(dsts):\n",
    "    kernel_width = .5\n",
    "    weights = np.exp(-(dsts**2)/kernel_width)\n",
    "    return weights\n",
    "\n",
    "knn = KNeighborsRegressor(n_neighbors=3, weights=gaussian)\n",
    "knn.fit(X_train, y_train)\n",
    "preds = knn.predict(X_test)\n",
    "plt.scatter(X_train,y_train, label=\"Training set\")\n",
    "plt.plot(X_test, preds, color=\"darkorange\", label=\"Predictions\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the impact of K vs weighting functions in a single plot combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 3, figsize=(12,10))\n",
    "for i, wfun in enumerate( [\"uniform\", \"distance\", gaussian]):\n",
    "    for j, k in enumerate([1,5,10]):\n",
    "        knn = KNeighborsRegressor(n_neighbors=k, weights=wfun) #weights=\"distance\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        preds = knn.predict(X_test)\n",
    "        s= wfun if wfun!=gaussian else \"gaussian\"\n",
    "        axs[i,j].set_title(\"Dfun:%s  (k=%d)\" %(s, k))\n",
    "        axs[i,j].scatter(X_train,y_train)\n",
    "        axs[i,j].plot(X_test, preds, color=\"darkorange\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2\n",
    "1. Comment and discuss the results above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6. Fitting KNN Regression to an actual data set\n",
    "\n",
    "Let's try the KNN Regression in a data set, using K=3, doing a full regression analysis.\n",
    "\n",
    "We will use the diabetes dataset for experimenting **Please note that we will use only 3 columns of X**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "X,y=load_diabetes(return_X_y=True)\n",
    "X=X[:,7:10] #<-----Just the 3 last columns\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will fit a KNN with K=3 and no distance weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, root_mean_squared_error, max_error, mean_absolute_error\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def present_reg_statistics(truth, preds):\n",
    "    print(\"The R2 is: \", r2_score(truth, preds))\n",
    "    print(\"The rmse is: \", root_mean_squared_error(truth, preds))\n",
    "    corr, pval = pearsonr(truth, preds)\n",
    "    print(\"The Correlation Score is is: %6.4f (p-value=%e)\\n\"%(corr,pval))\n",
    "    print(\"The Maximum Error is is: \", max_error(truth, preds))\n",
    "    print(\"The Mean Absolute Error is: \", mean_absolute_error(truth, preds))\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.scatter(preds, y_test)\n",
    "    plt.plot((50, 350), (50, 350), c=\"r\")\n",
    "    plt.ylabel(\"predictions\")\n",
    "    plt.xlabel(\"real values\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "knn = KNeighborsRegressor(n_neighbors=3 )\n",
    "knn.fit(X_train, y_train)\n",
    "preds = knn.predict(X_test)\n",
    "\n",
    "preds=knn.predict(X_test)\n",
    "\n",
    "present_reg_statistics(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's change K to investigate its impact on the RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks=np.arange(1,50,2)\n",
    "rmses=np.zeros(ks.shape[0])\n",
    "for i,k in enumerate(ks):\n",
    "    knn = KNeighborsRegressor(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    preds = knn.predict(X_test)\n",
    "    rmses[i]=root_mean_squared_error(y_test, preds)\n",
    "\n",
    "plt.plot(ks, rmses)\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.xlabel(\"K\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3\n",
    "\n",
    "1. With the results above what do you think is the best K?\n",
    "2. How confident are you in this result? \n",
    "3. With this \"optimal K\", compute all the regression and compare them to the original estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your Solution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "What is the difference if we use distance weighting for different values of K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks=np.arange(1,50,2)\n",
    "rmses_w=np.zeros(ks.shape[0])\n",
    "for i,k in enumerate(ks):\n",
    "    knn = KNeighborsRegressor(n_neighbors=k, weights=\"distance\")\n",
    "    knn.fit(X_train, y_train)\n",
    "    preds = knn.predict(X_test)\n",
    "    rmses_w[i]=root_mean_squared_error(y_test, preds)\n",
    "\n",
    "plt.plot(ks, rmses)\n",
    "plt.plot(ks, rmses_w, color=\"darkorange\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4\n",
    "\n",
    "1. Discuss the results above\n",
    "2. Run the code above, but this time run it with all the columns from X, by commenting out the line with \n",
    "`X=X[:,7:10]`\n",
    "    1. Discuss your findings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Scaling\n",
    "\n",
    "Data scaling is a procedure that is absolutely required for KNN modeling, as the distances are influenced directly by the scales of the atributes. Some of the most common scalers for Machine Learning are\n",
    "\n",
    "* [MinMax Scaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler) - scales each feature so that each feature will totally fit with a specifed interval (generally [0, 1])\n",
    "* [Standard Scaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler) This one transforms each column of the data set into a new feature with Mean = 0 and Variance =1\n",
    "* [Power Transformer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html#sklearn.preprocessing.PowerTransformer) - applies a power transformation to each value, so that the resulting feature has the best possible approach to a normal distribution \n",
    "* [Normalizer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html#sklearn.preprocessing.Normalizer) - Does not work on columns, but on rows, certifing that the norm of each intance is a unit vector (We won't be using this one in this TP)\n",
    "\n",
    "\n",
    "To test these transformations, we will use the [Algeria Forest Fire dataset](https://archive.ics.uci.edu/ml/datasets/Algerian+Forest+Fires+Dataset++#), but only 7 columns. Some of the values are missing, but most scalers will not be affected by this (with the exception of the Normalizer, that obviously cannot be processed without first imputing the missing elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alg=pd.read_csv(\"alg-ff1.txt\", sep=\"\\t\")\n",
    "X=alg.values[:,3:10]\n",
    "y=alg[\"Classes\"].values\n",
    "all_columns=alg.columns\n",
    "all_columns[np.array([0,1,2,10])]\n",
    "alg.drop(all_columns[np.array([0,1,2,10])], axis=1, inplace=True)\n",
    "pd.DataFrame(X, columns=alg.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "g = sns.PairGrid(alg)\n",
    "g.map_diag(sns.histplot)\n",
    "g.map_offdiag(sns.scatterplot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Running the MinMax Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, Normalizer, PowerTransformer\n",
    "scaler = MinMaxScaler()\n",
    "Xt=scaler.fit_transform(X)\n",
    "\n",
    "g = sns.PairGrid(pd.DataFrame(Xt, columns=alg.columns))\n",
    "g.map_diag(sns.histplot)\n",
    "g.map_offdiag(sns.scatterplot)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Running the Standard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "Xt=scaler.fit_transform(X)\n",
    "\n",
    "g = sns.PairGrid(pd.DataFrame(Xt, columns=alg.columns))\n",
    "g.map_diag(sns.histplot)\n",
    "g.map_offdiag(sns.scatterplot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Running the Power Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = PowerTransformer()\n",
    "Xt=scaler.fit_transform(X)\n",
    "\n",
    "g = sns.PairGrid(pd.DataFrame(Xt, columns=alg.columns))\n",
    "g.map_diag(sns.histplot)\n",
    "g.map_offdiag(sns.scatterplot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Imputation of missing values\n",
    "\n",
    "Two of the most common forms of imputation are Univariate imputation or Multivariate imputation. The former just puts in each missing value a constant, that is computed from the feature in question,typicallly  the mean, the median or the most common value (typically for categorical attributes)\n",
    "\n",
    "Multivariate imputation in Scikit can be performed with KNN, where for each feature with missing values a model is generated using all the other available features\n",
    "\n",
    "* [Univariate imputation](https://scikit-learn.org/stable/modules/impute.html#univariate-feature-imputation)\n",
    "* [KNN Imputer](https://scikit-learn.org/stable/modules/impute.html#nearest-neighbors-imputation)\n",
    "\n",
    "### 3.1. Univariate Imputation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "pd.DataFrame(imp.fit_transform(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. KNN Imputation\n",
    "\n",
    "This model is able to impute based on KNN models for each of the columns with missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "pd.DataFrame(imputer.fit_transform(X))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Putting it all together\n",
    "\n",
    "Now we are going to follow a Data Transformation and Classification Pipeline using KNN\n",
    "\n",
    "1. First we are going to split our data set into a training and testing set. We are just going to do simple cross validation. In a real world modeling problem, we would have to proceed accordingly\n",
    "2. Secondly we will scale our data using StandardScaler\n",
    "3. Then we will impute the missing values using Univariate Imputation with the mean.\n",
    "4. Then we will fit a KNN model with K=3\n",
    "5. Finally we will assess it's quality with the testing set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=123)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 2\n",
    "scaler   = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "Xt_train = scaler.transform(X_train)\n",
    "Xt_test  = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 3\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp.fit(Xt_train)\n",
    "Xt_train = imp.transform(Xt_train)\n",
    "Xt_test  = imp.transform(Xt_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step4\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(Xt_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 5\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, matthews_corrcoef, confusion_matrix\n",
    "\n",
    "def present_class_statistics(y_test, preds):\n",
    "    print(\"The Precision is: %7.4f\" % precision_score(y_test, preds))\n",
    "    print(\"The Recall is: %7.4f\" % recall_score(y_test, preds))\n",
    "    print(\"The F1 score is: %7.4f\" % f1_score(y_test, preds))\n",
    "    print(\"The Matthews correlation coefficient is: %7.4f\" % matthews_corrcoef(y_test, preds))\n",
    "    print()\n",
    "    print(\"This is the Confusion Matrix\")\n",
    "    print(pd.DataFrame(confusion_matrix(y_test, preds)))\n",
    "\n",
    "\n",
    "preds=knn.predict(Xt_test)\n",
    "present_class_statistics(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "\n",
    "1. Construct your data processing pipeline in such a way that it is possible to change the Model, Scaler and Imputer\n",
    "2. Using the same partition identify the Scaling algorithm that is able to get the best predictions\n",
    "3. Using the same partition identify the Imputation Algorithm that produces highest classification results\n",
    "4. Using distance weights increase the classification results?\n",
    "5. What about the number of Nearest Neighbours [If you have the time]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Solution\n",
    "\n",
    "# Suggestion: Construct runner()function so that it can run like this\n",
    "\n",
    "preds=runner(scaler, imp, knn, X_train, y_train, X_test)\n",
    "present_class_statistics(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
